{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epHtPjjmKzYr",
        "outputId": "28e6615c-9232-46cc-a504-fd9eecf5b401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.47.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.19.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.37.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install packages\n",
        "!pip install gradio PyPDF2 plotly openai\n",
        "\n",
        "# Cell 2: Paste the entire code from the artifact"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import PyPDF2\n",
        "import re\n",
        "from io import BytesIO\n",
        "import plotly.graph_objects as go\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Initialize models (will be loaded once)\n",
        "summarizer = None\n",
        "text_generator = None\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"Load Hugging Face models - runs once at startup\"\"\"\n",
        "    global summarizer, text_generator\n",
        "\n",
        "    try:\n",
        "        print(\"Loading AI models... This may take a minute on first run.\")\n",
        "\n",
        "        # Use a smaller, faster model for Colab\n",
        "        model_name = \"facebook/bart-large-cnn\"  # Good for summarization\n",
        "        summarizer = pipeline(\"summarization\", model=model_name, device=-1)  # CPU\n",
        "\n",
        "        # For text generation (resume enhancement)\n",
        "        gen_model_name = \"gpt2\"  # Lightweight and fast\n",
        "        text_generator = pipeline(\"text-generation\", model=gen_model_name, device=-1)\n",
        "\n",
        "        print(\"‚úÖ Models loaded successfully!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error loading models: {e}\")\n",
        "        return False\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    try:\n",
        "        if isinstance(pdf_file, bytes):\n",
        "            pdf_file = BytesIO(pdf_file)\n",
        "\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text, len(pdf_reader.pages)\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF: {str(e)}\", 0\n",
        "\n",
        "def analyze_document_synopsis(pdf_file, text):\n",
        "    \"\"\"Analyze document basic information\"\"\"\n",
        "    if isinstance(pdf_file, bytes):\n",
        "        file_size = len(pdf_file) / 1024\n",
        "        pdf_stream = BytesIO(pdf_file)\n",
        "    else:\n",
        "        file_size = len(pdf_file.getvalue()) / 1024\n",
        "        pdf_stream = pdf_file\n",
        "\n",
        "    _, page_count = extract_text_from_pdf(pdf_stream)\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    ats_score = 100\n",
        "    if page_count > 2:\n",
        "        ats_score -= 15\n",
        "    if file_size > 1024:\n",
        "        ats_score -= 10\n",
        "\n",
        "    return {\n",
        "        \"ATS Compliance\": f\"{ats_score}%\",\n",
        "        \"File Type\": \"PDF\",\n",
        "        \"File Size\": f\"{file_size:.2f} KB\",\n",
        "        \"Page Count\": page_count,\n",
        "        \"Word Count\": word_count\n",
        "    }\n",
        "\n",
        "def analyze_data_identification(text):\n",
        "    \"\"\"Identify key data points in resume\"\"\"\n",
        "    phone_pattern = r'\\b(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    linkedin_pattern = r'(linkedin\\.com/in/[A-Za-z0-9_-]+|linkedin\\.com/pub/[A-Za-z0-9_-]+)'\n",
        "\n",
        "    phone = re.findall(phone_pattern, text)\n",
        "    email = re.findall(email_pattern, text)\n",
        "    linkedin = re.findall(linkedin_pattern, text, re.IGNORECASE)\n",
        "\n",
        "    education = bool(re.search(r'\\b(education|degree|university|college|bachelor|master)\\b', text, re.IGNORECASE))\n",
        "    work_history = bool(re.search(r'\\b(experience|work history|employment|position|job)\\b', text, re.IGNORECASE))\n",
        "    skills = bool(re.search(r'\\b(skills|competencies|expertise|proficient)\\b', text, re.IGNORECASE))\n",
        "\n",
        "    date_patterns = [\n",
        "        r'\\b\\d{4}\\s*-\\s*\\d{4}\\b',\n",
        "        r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}\\b',\n",
        "        r'\\b\\d{1,2}/\\d{4}\\b'\n",
        "    ]\n",
        "    dates_found = any(re.search(pattern, text, re.IGNORECASE) for pattern in date_patterns)\n",
        "\n",
        "    return {\n",
        "        \"Phone Number\": \"‚úì Found\" if phone else \"‚úó Missing\",\n",
        "        \"E-mail Address\": \"‚úì Found\" if email else \"‚úó Missing\",\n",
        "        \"LinkedIn URL\": \"‚úì Found\" if linkedin else \"‚úó Missing\",\n",
        "        \"Education\": \"‚úì Found\" if education else \"‚úó Missing\",\n",
        "        \"Work History\": \"‚úì Found\" if work_history else \"‚úó Missing\",\n",
        "        \"Skills / Achievements\": \"‚úì Found\" if skills else \"‚úó Missing\",\n",
        "        \"Date Formatting\": \"‚úì Consistent\" if dates_found else \"‚úó Inconsistent\"\n",
        "    }\n",
        "\n",
        "def analyze_lexical(text):\n",
        "    \"\"\"Perform lexical analysis\"\"\"\n",
        "    words = text.split()\n",
        "    total_words = len(words)\n",
        "\n",
        "    pronouns = r'\\b(I|me|my|mine|we|us|our|ours)\\b'\n",
        "    pronoun_count = len(re.findall(pronouns, text, re.IGNORECASE))\n",
        "    pronoun_percentage = (pronoun_count / total_words * 100) if total_words > 0 else 0\n",
        "\n",
        "    numbers = re.findall(r'\\b\\d+[%]?\\b', text)\n",
        "    numeric_percentage = (len(numbers) / total_words * 100) if total_words > 0 else 0\n",
        "\n",
        "    unique_words = len(set(word.lower() for word in words if word.isalpha()))\n",
        "    vocab_ratio = (unique_words / total_words * 100) if total_words > 0 else 0\n",
        "\n",
        "    avg_word_length = sum(len(word) for word in words if word.isalpha()) / len([w for w in words if w.isalpha()]) if words else 0\n",
        "    reading_level = \"Advanced\" if avg_word_length > 6 else \"Intermediate\" if avg_word_length > 4 else \"Basic\"\n",
        "\n",
        "    power_words = ['achieved', 'improved', 'increased', 'developed', 'managed', 'led', 'created', 'designed', 'implemented']\n",
        "    power_word_count = sum(1 for word in power_words if word in text.lower())\n",
        "\n",
        "    return {\n",
        "        \"Personal Pronouns\": f\"{pronoun_percentage:.1f}% (Lower is better)\",\n",
        "        \"Numericized Data\": f\"{numeric_percentage:.1f}% ({len(numbers)} metrics found)\",\n",
        "        \"Vocabulary Level\": f\"{vocab_ratio:.1f}% unique words\",\n",
        "        \"Reading Level\": reading_level,\n",
        "        \"Power Words\": f\"{power_word_count} action words found\"\n",
        "    }\n",
        "\n",
        "def analyze_semantic(text):\n",
        "    \"\"\"Perform semantic analysis\"\"\"\n",
        "    words = text.split()\n",
        "    total_words = len(words)\n",
        "\n",
        "    achievement_pattern = r'\\b\\d+[%]?\\s*(increase|decrease|improve|reduce|save|generate|grow|boost)\\w*\\b'\n",
        "    achievements = re.findall(achievement_pattern, text, re.IGNORECASE)\n",
        "    achievement_count = len(achievements)\n",
        "\n",
        "    common_hard_skills = [\n",
        "        'python', 'java', 'javascript', 'sql', 'aws', 'azure', 'docker', 'kubernetes',\n",
        "        'machine learning', 'data analysis', 'project management', 'agile', 'scrum',\n",
        "        'excel', 'powerpoint', 'salesforce', 'tableau', 'react', 'node.js', 'git',\n",
        "        'html', 'css', 'mongodb', 'postgresql', 'tensorflow', 'pytorch', 'spark'\n",
        "    ]\n",
        "    hard_skills_found = [skill for skill in common_hard_skills if skill.lower() in text.lower()]\n",
        "    hard_skills_count = len(hard_skills_found)\n",
        "\n",
        "    skills_ratio = (hard_skills_count / (total_words / 100)) if total_words > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"Measurable Achievements\": f\"{achievement_count} quantified results\",\n",
        "        \"Hard Skills Identified\": f\"{hard_skills_count} technical skills\",\n",
        "        \"Skills Efficiency Ratio\": f\"{skills_ratio:.2f}\",\n",
        "        \"Top Skills\": \", \".join(hard_skills_found[:5]) if hard_skills_found else \"None detected\"\n",
        "    }\n",
        "\n",
        "def calculate_ats_score(synopsis, identification, lexical, semantic):\n",
        "    \"\"\"Calculate overall ATS score\"\"\"\n",
        "    score = 0\n",
        "\n",
        "    if int(synopsis[\"Page Count\"]) <= 2:\n",
        "        score += 10\n",
        "    if int(synopsis[\"Word Count\"]) >= 300:\n",
        "        score += 10\n",
        "\n",
        "    identification_checks = [v for v in identification.values() if \"‚úì\" in str(v)]\n",
        "    score += len(identification_checks) * 4.3\n",
        "\n",
        "    pronoun_val = float(lexical[\"Personal Pronouns\"].split(\"%\")[0])\n",
        "    if pronoun_val < 2:\n",
        "        score += 10\n",
        "    numeric_val = float(lexical[\"Numericized Data\"].split(\"%\")[0])\n",
        "    if numeric_val > 5:\n",
        "        score += 10\n",
        "    if lexical[\"Reading Level\"] in [\"Intermediate\", \"Advanced\"]:\n",
        "        score += 5\n",
        "\n",
        "    achievement_count = int(semantic[\"Measurable Achievements\"].split()[0])\n",
        "    if achievement_count >= 3:\n",
        "        score += 10\n",
        "    skills_count = int(semantic[\"Hard Skills Identified\"].split()[0])\n",
        "    if skills_count >= 5:\n",
        "        score += 10\n",
        "    if skills_count >= 10:\n",
        "        score += 5\n",
        "\n",
        "    return min(int(score), 100)\n",
        "\n",
        "def get_ai_analysis(text, synopsis, identification, lexical, semantic):\n",
        "    \"\"\"Generate AI analysis using rule-based insights + AI enhancement\"\"\"\n",
        "\n",
        "    # Rule-based analysis\n",
        "    issues = []\n",
        "    strengths = []\n",
        "\n",
        "    # Check issues\n",
        "    if int(synopsis[\"Page Count\"]) > 2:\n",
        "        issues.append(\"Resume is longer than 2 pages - consider condensing\")\n",
        "\n",
        "    if \"‚úó\" in identification[\"Phone Number\"]:\n",
        "        issues.append(\"Missing phone number - add contact information\")\n",
        "    if \"‚úó\" in identification[\"E-mail Address\"]:\n",
        "        issues.append(\"Missing email address - essential for contact\")\n",
        "    if \"‚úó\" in identification[\"LinkedIn URL\"]:\n",
        "        issues.append(\"Missing LinkedIn URL - highly recommended for networking\")\n",
        "\n",
        "    pronoun_val = float(lexical[\"Personal Pronouns\"].split(\"%\")[0])\n",
        "    if pronoun_val > 5:\n",
        "        issues.append(f\"Too many personal pronouns ({pronoun_val:.1f}%) - use third person or remove them\")\n",
        "\n",
        "    numeric_val = float(lexical[\"Numericized Data\"].split(\"%\")[0])\n",
        "    if numeric_val < 5:\n",
        "        issues.append(\"Lack of quantifiable achievements - add numbers, percentages, metrics\")\n",
        "\n",
        "    achievement_count = int(semantic[\"Measurable Achievements\"].split()[0])\n",
        "    if achievement_count < 3:\n",
        "        issues.append(\"Few measurable achievements - quantify your impact with specific results\")\n",
        "\n",
        "    skills_count = int(semantic[\"Hard Skills Identified\"].split()[0])\n",
        "    if skills_count < 5:\n",
        "        issues.append(\"Limited technical skills listed - add relevant hard skills for your field\")\n",
        "\n",
        "    # Check strengths\n",
        "    if int(synopsis[\"Page Count\"]) <= 2:\n",
        "        strengths.append(\"Good length - concise and focused\")\n",
        "\n",
        "    if numeric_val > 10:\n",
        "        strengths.append(\"Strong use of metrics and quantifiable data\")\n",
        "\n",
        "    if achievement_count >= 5:\n",
        "        strengths.append(\"Excellent track record of measurable achievements\")\n",
        "\n",
        "    if skills_count >= 10:\n",
        "        strengths.append(\"Comprehensive technical skills section\")\n",
        "\n",
        "    if pronoun_val < 2:\n",
        "        strengths.append(\"Professional tone - minimal use of personal pronouns\")\n",
        "\n",
        "    # Generate analysis text\n",
        "    analysis = f\"\"\"## üéØ Overall Assessment\n",
        "\n",
        "{\"Your resume shows strong fundamentals with good ATS optimization.\" if len(strengths) > len(issues) else \"Your resume needs improvement in several key areas for better ATS performance.\"}\n",
        "\n",
        "## ‚úÖ Key Strengths ({len(strengths)})\n",
        "\"\"\"\n",
        "\n",
        "    for i, strength in enumerate(strengths[:5], 1):\n",
        "        analysis += f\"{i}. {strength}\\n\"\n",
        "\n",
        "    if not strengths:\n",
        "        analysis += \"- Consider implementing the improvements below to strengthen your resume\\n\"\n",
        "\n",
        "    analysis += f\"\"\"\n",
        "## ‚ö†Ô∏è Areas for Improvement ({len(issues)})\n",
        "\"\"\"\n",
        "\n",
        "    for i, issue in enumerate(issues[:5], 1):\n",
        "        analysis += f\"{i}. {issue}\\n\"\n",
        "\n",
        "    analysis += \"\"\"\n",
        "## üöÄ ATS Optimization Tips\n",
        "\n",
        "1. **Use Action Verbs**: Start bullet points with powerful verbs (Achieved, Developed, Led, Implemented)\n",
        "2. **Quantify Everything**: Add numbers wherever possible (%, $, time saved, team size)\n",
        "3. **Keywords Matter**: Include industry-specific terms and skills from job descriptions\n",
        "4. **Format Simply**: Use standard fonts, avoid tables/graphics that ATS can't read\n",
        "5. **Customize**: Tailor your resume for each job application\n",
        "\n",
        "## üí° Quick Wins\n",
        "\n",
        "- Add 3-5 more quantifiable achievements to your bullet points\n",
        "- Include specific metrics (increased by X%, reduced by Y hours)\n",
        "- List technical skills relevant to your target role\n",
        "- Ensure all contact information is present and formatted correctly\n",
        "\"\"\"\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def get_enhanced_bullets(text):\n",
        "    \"\"\"Generate enhanced bullet point examples\"\"\"\n",
        "\n",
        "    # Extract existing bullet points\n",
        "    bullet_pattern = r'[‚Ä¢\\-\\*]\\s*(.+?)(?=\\n[‚Ä¢\\-\\*]|\\n\\n|\\Z)'\n",
        "    bullets = re.findall(bullet_pattern, text, re.DOTALL)\n",
        "\n",
        "    if not bullets:\n",
        "        return \"\"\"## ‚ú® Enhanced Bullet Point Examples\n",
        "\n",
        "Since no bullet points were detected, here are examples of strong resume bullets:\n",
        "\n",
        "**Weak**: Responsible for managing team\n",
        "**Strong**: Led cross-functional team of 12 developers, reducing project delivery time by 30% and increasing customer satisfaction scores from 3.2 to 4.5/5\n",
        "\n",
        "**Weak**: Worked on sales activities\n",
        "**Strong**: Drove $2.5M in new revenue by identifying and closing 45+ enterprise accounts, exceeding quarterly quota by 140%\n",
        "\n",
        "**Weak**: Improved system performance\n",
        "**Strong**: Optimized database queries and API endpoints, reducing average response time from 2.5s to 400ms and improving user retention by 25%\n",
        "\n",
        "### Tips for Writing Strong Bullets:\n",
        "1. Start with an action verb (Led, Developed, Achieved, Increased)\n",
        "2. Add specific numbers and metrics\n",
        "3. Show impact and results, not just responsibilities\n",
        "4. Use industry-specific keywords\n",
        "\"\"\"\n",
        "\n",
        "    enhancement = \"## ‚ú® Enhanced Bullet Point Examples\\n\\n\"\n",
        "    enhancement += \"Here are your bullets rewritten with stronger impact:\\n\\n\"\n",
        "\n",
        "    # Take first 3 bullets and enhance them\n",
        "    action_verbs = ['Achieved', 'Developed', 'Led', 'Implemented', 'Increased', 'Reduced', 'Optimized', 'Drove']\n",
        "\n",
        "    for i, bullet in enumerate(bullets[:3], 1):\n",
        "        original = bullet.strip()[:100]  # Limit length\n",
        "\n",
        "        enhancement += f\"**Original {i}**: {original}\\n\\n\"\n",
        "        enhancement += f\"**Enhanced {i}**: \"\n",
        "\n",
        "        # Add action verb if missing\n",
        "        if not any(original.lower().startswith(verb.lower()) for verb in action_verbs):\n",
        "            enhancement += f\"Achieved {original.lower()}\"\n",
        "        else:\n",
        "            enhancement += original\n",
        "\n",
        "        enhancement += \" [Add specific metrics: numbers, percentages, timeframes, or dollar amounts here]\\n\\n\"\n",
        "\n",
        "    enhancement += \"\"\"\n",
        "### Enhancement Tips:\n",
        "- Add quantifiable results (increased by 40%, saved $50K, reduced time by 2 hours)\n",
        "- Include scope (team size, budget, project duration)\n",
        "- Show business impact (revenue, efficiency, customer satisfaction)\n",
        "- Use strong action verbs at the start\n",
        "\"\"\"\n",
        "\n",
        "    return enhancement\n",
        "\n",
        "def create_ats_chart(ats_score):\n",
        "    \"\"\"Create ATS score visualization\"\"\"\n",
        "    remaining = 100 - ats_score\n",
        "    colors = ['#10b981' if ats_score >= 70 else '#f59e0b' if ats_score >= 50 else '#ef4444', '#e5e7eb']\n",
        "\n",
        "    fig = go.Figure(data=[go.Pie(\n",
        "        labels=['ATS Score', 'Gap'],\n",
        "        values=[ats_score, remaining],\n",
        "        hole=.6,\n",
        "        marker=dict(colors=colors),\n",
        "        textinfo='none',\n",
        "        hoverinfo='label+percent'\n",
        "    )])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=dict(text=f\"<b>ATS Score: {ats_score}/100</b>\", x=0.5, xanchor='center', font=dict(size=24)),\n",
        "        annotations=[dict(text=f'<b>{ats_score}</b><br>ATS Score', x=0.5, y=0.5, font_size=32, showarrow=False)],\n",
        "        showlegend=True,\n",
        "        height=400,\n",
        "        margin=dict(t=80, b=20, l=20, r=20)\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def format_results(data_dict):\n",
        "    \"\"\"Format results as HTML table\"\"\"\n",
        "    html = \"<table style='width:100%; border-collapse: collapse;'>\"\n",
        "    for key, value in data_dict.items():\n",
        "        html += f\"<tr style='border-bottom: 1px solid #ddd;'>\"\n",
        "        html += f\"<td style='padding: 10px; font-weight: bold; width: 40%;'>{key}</td>\"\n",
        "        html += f\"<td style='padding: 10px;'>{value}</td>\"\n",
        "        html += \"</tr>\"\n",
        "    html += \"</table>\"\n",
        "    return html\n",
        "\n",
        "def analyze_resume(pdf_file):\n",
        "    \"\"\"Main analysis function\"\"\"\n",
        "    if pdf_file is None:\n",
        "        return \"Please upload a PDF file\", \"\", \"\", \"\", \"\", None, \"\", \"\"\n",
        "\n",
        "    # Extract text\n",
        "    text, _ = extract_text_from_pdf(pdf_file)\n",
        "\n",
        "    if \"Error\" in text:\n",
        "        return text, \"\", \"\", \"\", \"\", None, \"\", \"\"\n",
        "\n",
        "    # Perform analyses\n",
        "    synopsis = analyze_document_synopsis(pdf_file, text)\n",
        "    identification = analyze_data_identification(text)\n",
        "    lexical = analyze_lexical(text)\n",
        "    semantic = analyze_semantic(text)\n",
        "\n",
        "    # Calculate ATS score\n",
        "    ats_score = calculate_ats_score(synopsis, identification, lexical, semantic)\n",
        "\n",
        "    # Create chart\n",
        "    chart = create_ats_chart(ats_score)\n",
        "\n",
        "    # Get AI analysis (rule-based + insights)\n",
        "    ai_analysis = get_ai_analysis(text, synopsis, identification, lexical, semantic)\n",
        "    enhanced_bullets = get_enhanced_bullets(text)\n",
        "\n",
        "    # Format results\n",
        "    synopsis_html = format_results(synopsis)\n",
        "    identification_html = format_results(identification)\n",
        "    lexical_html = format_results(lexical)\n",
        "    semantic_html = format_results(semantic)\n",
        "\n",
        "    # Summary\n",
        "    summary = f\"\"\"\n",
        "    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; color: white; margin-bottom: 20px;'>\n",
        "        <h2 style='margin: 0; font-size: 28px;'>üìä Overall ATS Score: {ats_score}/100</h2>\n",
        "        <p style='margin: 10px 0 0 0; font-size: 16px;'>\n",
        "            {'‚úÖ Excellent! Your resume is ATS-optimized.' if ats_score >= 70 else\n",
        "             '‚ö†Ô∏è Good, but needs improvement.' if ats_score >= 50 else\n",
        "             '‚ùå Needs significant optimization.'}\n",
        "        </p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    return summary, synopsis_html, identification_html, lexical_html, semantic_html, chart, ai_analysis, enhanced_bullets\n",
        "\n",
        "# Create Gradio Interface\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"AI Resume Analyzer\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üéØ AI Resume Analyzer & ATS Score Calculator\n",
        "    ##\n",
        "\n",
        "    Upload your resume (PDF format) to get a comprehensive ATS analysis with AI-powered insights.\n",
        "    This uses advanced rule-based AI analysis - completely free and runs locally!\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            pdf_input = gr.File(\n",
        "                label=\"üìÑ Upload Your Resume (PDF)\",\n",
        "                file_types=[\".pdf\"],\n",
        "                type=\"binary\"\n",
        "            )\n",
        "            analyze_btn = gr.Button(\"üîç Analyze Resume\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "    summary_output = gr.HTML(label=\"Summary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            chart_output = gr.Plot(label=\"ATS Score Visualization\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"ü§ñ AI Analysis & Recommendations\"):\n",
        "            ai_analysis_output = gr.Markdown(label=\"Detailed Analysis\")\n",
        "\n",
        "        with gr.Tab(\"‚ú® Enhanced Bullet Points\"):\n",
        "            enhanced_bullets_output = gr.Markdown(label=\"Before & After Examples\")\n",
        "\n",
        "        with gr.Tab(\"üìã Document Synopsis\"):\n",
        "            synopsis_output = gr.HTML()\n",
        "\n",
        "        with gr.Tab(\"üîç Data Identification\"):\n",
        "            identification_output = gr.HTML()\n",
        "\n",
        "        with gr.Tab(\"üìù Lexical Analysis\"):\n",
        "            lexical_output = gr.HTML()\n",
        "\n",
        "        with gr.Tab(\"üß† Semantic Analysis\"):\n",
        "            semantic_output = gr.HTML()\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    ### üìä Scoring Criteria:\n",
        "    - **Document Synopsis**: File optimization, page count, word count\n",
        "    - **Data Identification**: Contact info, sections completeness\n",
        "    - **Lexical Analysis**: Professional language, quantified data\n",
        "    - **Semantic Analysis**: Measurable achievements, hard skills\n",
        "\n",
        "    ### üí° Features:\n",
        "    - ‚úÖ 100% Free\n",
        "    - ‚úÖ AI-powered analysis and recommendations\n",
        "    - ‚úÖ ATS optimization tips\n",
        "    - ‚úÖ Enhanced bullet point examples\n",
        "    - ‚úÖ Comprehensive scoring across 4 dimensions\n",
        "    \"\"\")\n",
        "\n",
        "    analyze_btn.click(\n",
        "        fn=analyze_resume,\n",
        "        inputs=[pdf_input],\n",
        "        outputs=[summary_output, synopsis_output, identification_output,\n",
        "                lexical_output, semantic_output, chart_output,\n",
        "                ai_analysis_output, enhanced_bullets_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m0YXX5oVOCSm",
        "outputId": "f055a685-dd47-4f8c-99a7-085a78831278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d709fd6f55e6e9a6b8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d709fd6f55e6e9a6b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 387, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x783457732b40 [unset]> is bound to a different event loop\n"
          ]
        }
      ]
    }
  ]
}